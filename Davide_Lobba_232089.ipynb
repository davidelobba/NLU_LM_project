{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLU project Language Modeling (LM)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Davide Lobba - 232089"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Language modeling is a fundamental task in Natural Language Processing (NLP) that aims to predict the likelihood of words in a sentence, as well as generate new words based on a given context.\n",
        "The goal of this task is to train a model that can produce new meaningful text according to the user's needs. It is essential to train the model on a massive corpus of text, such as books, articles or web pages. This is because, during the training phase, the language model learns the dependencies and probability distributions between words, which enables it to generate text that is coherent and meaningful.\n",
        "\n",
        "The dataset used for training and evaluating the models used in this project is a preprocessed version of the Penn Treebank (PTB). This version of PTB contains only lower-cased words, numbers are replaced with N, rare words are replaced by unk and the vocabulary consists of the most frequent 10k words. This is one of the most popular datasets used by the NLP community, and it is used for both character-level and word-level language modeling. The dataset was created by the University of Pennsylvania by collecting articles from the Wall Street Journal.\n",
        "\n",
        "The models presented in this project are:\n",
        "- LSTM vanilla\n",
        "- LSTM with improvements proposed by the paper [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182)\n",
        "- GPT-2 provided by Hugging Face, specifically zero-shot analysis and finetuning on PTB dataset\n",
        "\n",
        "The evaluation metric used to evaluate the performance of the models is the perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LUDzqO2WgxT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90JjaB9NW7RO"
      },
      "outputs": [],
      "source": [
        "!wget 'https://data.deepai.org/ptbdataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B6scbm_XfFb"
      },
      "outputs": [],
      "source": [
        "!mkdir dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rUr8cirXvo1"
      },
      "outputs": [],
      "source": [
        "!unzip /content/ptbdataset.zip -d dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-_ZQprXVq_1"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pytorch_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ri2fpzTVuH9"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(relogin=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOy5lnQGWgxX"
      },
      "outputs": [],
      "source": [
        "PAD_TOKEN = 0\n",
        "\n",
        "class Lang():\n",
        "    '''\n",
        "    This class contains methods for creating word-to-id and label-to-id mappings.\n",
        "        The w2id method maps words to unique IDs.\n",
        "        The lab2id method maps labels to unique IDs.\n",
        "    '''\n",
        "    def __init__(self, words, cutoff=0):\n",
        "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True, eos=True)\n",
        "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
        "        \n",
        "    def w2id(self, elements, cutoff=None, unk=True, eos=True):\n",
        "        vocab = {'pad': PAD_TOKEN, '<unk>': 1, '<eos>': 2}\n",
        "        count = Counter(elements)\n",
        "        for k, v in count.items():\n",
        "            if v > cutoff:\n",
        "                vocab[k] = len(vocab)\n",
        "        return vocab\n",
        "    \n",
        "    def lab2id(self, elements, pad=True):\n",
        "        vocab = {}\n",
        "        if pad:\n",
        "            vocab['pad'] = PAD_TOKEN\n",
        "        for elem in elements:\n",
        "                vocab[elem] = len(vocab)\n",
        "        return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO0odEBKTq5k"
      },
      "source": [
        "## Functions for document handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9a2eRpvWgxZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "def read_file(path):\n",
        "    with open(path, 'r') as f:\n",
        "        words = f.read().split()\n",
        "    return words\n",
        "\n",
        "def read_sentence(sent):\n",
        "    words = sent.split()\n",
        "    return words\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, path, lang):\n",
        "        self.input = []\n",
        "        self.target = []\n",
        "        self.length = []\n",
        "        \n",
        "        with open(path, 'r') as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                words = [lang.word2id[word] for word in words]\n",
        "                self.input.append(words[:-1])\n",
        "                self.target.append(words[1:])\n",
        "                self.length.append(len(words[:-1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = torch.Tensor(self.input[idx])\n",
        "        target = torch.Tensor(self.target[idx])\n",
        "        length = self.length[idx]\n",
        "        return input, target, length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7da7DlETq5p"
      },
      "source": [
        "## Collate function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ1p0iKgTq5q"
      },
      "source": [
        "This collate function is used to prepare a batch of samples for training, test or evaluation. By using this function, we can make sure that the sequences in the same batch have the same length, and the model can process them efficiently, and it's also handling the input and target sequences separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rABy4KbFWgxc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def collate_fn(data):\n",
        "    def merge(sequences):\n",
        "        '''\n",
        "        merge from batch * sent_len to batch * max_len \n",
        "        '''\n",
        "        lengths = [len(seq) for seq in sequences]\n",
        "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
        "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
        "        for i, seq in enumerate(sequences):\n",
        "            end = lengths[i]\n",
        "            padded_seqs[i, :end] = seq\n",
        "        \n",
        "        padded_seqs = padded_seqs.detach()\n",
        "        \n",
        "        return padded_seqs, lengths\n",
        "        \n",
        "    input_test = []\n",
        "    target_test = []\n",
        "    for inputs, target, length in data:\n",
        "        input_test.append(inputs)\n",
        "        target_test.append(target)\n",
        "        inputs, length = merge(input_test)\n",
        "        target, _ = merge(target_test)\n",
        "    \n",
        "    return inputs, target, length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jANY67nKTq5s"
      },
      "source": [
        "## Baseline LSTM\n",
        "\n",
        "This class defines a Long Short-Term Memory (LSTM) model. LSTM is a type of Recurrent Neural Network (RNN) that is commonly used in NLP tasks, such as language modeling and text classification.\n",
        "The class architecture consists of an embedding layer, an LSTM layer and an output layer which is a fully connected layer.\n",
        "The embedding layer is used to obtain the vector representation of every word, the LSTM layer processes the sentence and then the fully connected layer is useful for the prediction of the class probability for each word. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8SVSEKrWgxd"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch import nn\n",
        "from typing import List, Tuple, Union\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class BaselineLSTM(nn.Module):\n",
        "    '''\n",
        "    This class is the implementation of a BaselineLSTM.\n",
        "        num_classes: number of classes\n",
        "        emb_dim: embedding dimension for the feature representation of the words\n",
        "        hid_dim: dimension of the hidden layer of the LSTM\n",
        "        n_layers: number of hidden layers of the LSTM\n",
        "        pad_value: value used for the padding of the sequences\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_classes, emb_dim, hid_dim, n_layers = 1, pad_value = 0):\n",
        "        super(BaselineLSTM, self).__init__()\n",
        "        self.embd = nn.Embedding(num_classes, emb_dim, pad_value)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, bidirectional=False)\n",
        "        self.classifier = nn.Linear(hid_dim, num_classes)\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hid_dim\n",
        "        self.pad_value = pad_value\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.data.ndimension() >= 2:\n",
        "                nn.init.xavier_uniform_(p.data)\n",
        "            else:\n",
        "                nn.init.zeros_(p.data)\n",
        "\n",
        "    def forward(self, input, lengths, hidden = None):\n",
        "        total_length = input.shape[1] #because input is [batch_size, sequence, ]\n",
        "        input_emb = self.embd(input)\n",
        "\n",
        "        packed_input = pack_padded_sequence(input_emb, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hidden = self.lstm(packed_input)\n",
        "\n",
        "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True, padding_value=self.pad_value, total_length=total_length)\n",
        "\n",
        "        output = self.classifier(output)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        output = output.reshape(output.size(0)*output.size(1), output.size(2))\n",
        "\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmElQiflTq5u"
      },
      "source": [
        "## Training and Test functions\n",
        "\n",
        "The loss function used for language modeling task is the perplexity. Perplexity is computed as follow: \n",
        "$$ \\text{PP(W)} = e^{\\mathrm{L_{CE}(W)}}$$\n",
        "\n",
        "- Batch size for train is 32 and for test and evaluation is 16\n",
        "- The optimizer used is the Stochasthic Gradient Descent with LR=1.0\n",
        "- The scheduler used is the ReduceLROnPlateau\n",
        "- The LSTM is implemented with only 1 hidden layer\n",
        "- Dimension of embedding and hidden layer is 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEN5X9ubWgxf"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "def train_step(model, optimizer, loss_function, train_loader, device, clip = None, scheduler = None, valid_perplexity = None):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (inputs, targets, lengths) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs, hidden = model(inputs, lengths)\n",
        "        loss = loss_function(outputs, targets.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if clip is not None:\n",
        "            clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    loss = total_loss/(i+1)\n",
        "    perplexity = math.exp(loss)\n",
        "\n",
        "    if scheduler is not None:\n",
        "        scheduler.step(valid_perplexity)\n",
        "\n",
        "    return loss, perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez38uqQAWgxg"
      },
      "outputs": [],
      "source": [
        "def test_step(model, loss_function, test_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets, lengths) in enumerate(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs, hidden = model(inputs, lengths)\n",
        "            loss = loss_function(outputs, targets.view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    loss = total_loss/(i+1)\n",
        "    perplexity = math.exp(loss)\n",
        "\n",
        "    return loss, perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0AAScIVWgxh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, optimizer, loss_function, train_loader, test_loader, valid_loader, epoch, device, clip = None, scheduler = None, ntsgd = True):\n",
        "    perplexity = float('inf')\n",
        "    valid_perplexity = float('inf')\n",
        "    valid_perplexity_list = []\n",
        "    n=5\n",
        "    \n",
        "    for e in tqdm(range(epoch)):\n",
        "        train_loss, train_perplexity = train_step(model, optimizer, loss_function, train_loader, device, clip, scheduler, valid_perplexity)\n",
        "        \n",
        "        test_loss, test_perplexity = test_step(model, loss_function, test_loader, device)\n",
        "\n",
        "        valid_loss, valid_perplexity = test_step(model, loss_function, valid_loader, device)\n",
        "\n",
        "        print(\"\\nEpoch: {}\".format(e+1))\n",
        "        print(\"Train Loss : {}, Train Perplexity : {}\" .format(train_loss, train_perplexity))\n",
        "        print(\"Test Loss : {}, Test Perplexity : {}\" .format(test_loss, test_perplexity))\n",
        "        print(\"Valid Loss : {}, Valid Perplexity : {}\" .format(valid_loss, valid_perplexity))\n",
        "\n",
        "        valid_perplexity_list.append(valid_perplexity)\n",
        "\n",
        "        if ntsgd and 't0' not in optimizer.param_groups[0] and (len(valid_perplexity_list)>n and valid_perplexity > min(valid_perplexity_list[:-n])):\n",
        "                print('Switching to ASGD')\n",
        "                optimizer = torch.optim.ASGD(model.parameters(), lr=1, t0=0, lambd=0., weight_decay=1.2e-6)\n",
        "\n",
        "        if valid_perplexity < perplexity:\n",
        "            os.makedirs(\"/content/lstm_weights/\", exist_ok = True)\n",
        "            torch.save(model.state_dict(), \"/content/lstm_weights/lstm.pt\")\n",
        "            perplexity = valid_perplexity\n",
        "        \n",
        "        wandb.log({\"Epoch\": e+1, \"train_perplexity\": train_perplexity, \"test_perplexity\": test_perplexity, \"valid_perplexity\": valid_perplexity})\n",
        "    wandb.log({'best_perplexity': perplexity})\n",
        "    wandb.run.summary[\"best_perplexity\"] = perplexity\n",
        "    wandb.finish()\n",
        "\n",
        "    print(\"\\nTraining Completed, the best perplexity reached in validation is : {}\".format(perplexity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W-ZvnavWgxj"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def main_baseline(hid_size, emb_size, n_layers, lr, pad_value, epochs):\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    data_train = read_file('dataset/ptb.train.txt')\n",
        "    vocab = Lang(data_train)\n",
        "\n",
        "    train_dataset = Dataset('dataset/ptb.train.txt', vocab)\n",
        "    test_dataset = Dataset('dataset/ptb.test.txt', vocab)\n",
        "    valid_dataset = Dataset('dataset/ptb.valid.txt', vocab)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "    vocab_len = len(vocab.word2id)\n",
        "\n",
        "    model = BaselineLSTM(vocab_len, emb_size, hid_size, n_layers = n_layers, pad_value = 0).to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=pad_value)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
        "  \n",
        "    wandb.init(project=\"NLU_project\",\n",
        "               name = \"LSTM baseline\",\n",
        "               config = {\n",
        "               \"learning_rate\": lr,\n",
        "               \"architecture\": \"Baseline\",\n",
        "               \"hidden_size\": hid_size,\n",
        "               \"embedding_size\": emb_size,\n",
        "               \"epochs\": epochs},\n",
        "               notes = \"this is the run of the baseline\"\n",
        "               )\n",
        "    \n",
        "    train(model, optimizer, loss_function, train_loader, test_loader, valid_loader, epochs, device, clip=None, scheduler=scheduler, ntsgd=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww8iFSPTTq5y"
      },
      "outputs": [],
      "source": [
        "main_baseline(hid_size=300, emb_size=300, lr=1.0, n_layers=1, pad_value=0, epochs=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRHLNb5mTq5z"
      },
      "source": [
        "## Variational dropout (Locked dropout) and Weight dropout\n",
        "Classes used to improve the LSTM model following the paper \"Merity, S., Keskar, N. S., & Socher, R. (2017). Regularizing and Optimizing LSTM Language Models. arXiv. https://doi.org/10.48550/arXiv.1708.02182\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BGlcM7gTq5z"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class LockedDropout(nn.Module):\n",
        "    '''\n",
        "    Class for the implementation of Variational dropout.\n",
        "    The class is taken from: https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/lock_dropout.html\n",
        "    '''\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        if not self.training or not self.p:\n",
        "            return x\n",
        "        x = x.clone()\n",
        "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n",
        "        mask = mask.div_(1 - self.p)\n",
        "        mask = mask.expand_as(x)\n",
        "        return x * mask\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'p=' + str(self.p) + ')'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzRwiBo2Tq50"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Parameter\n",
        "class BackHook(torch.nn.Module):\n",
        "    def __init__(self, hook):\n",
        "        super(BackHook, self).__init__()\n",
        "        self._hook = hook\n",
        "        self.register_backward_hook(self._backward)\n",
        "\n",
        "    def forward(self, *inp):\n",
        "        return inp\n",
        "\n",
        "    @staticmethod\n",
        "    def _backward(self, grad_in, grad_out):\n",
        "        self._hook()\n",
        "        return None\n",
        "\n",
        "\n",
        "class WeightDrop(torch.nn.Module):\n",
        "    '''\n",
        "    Class for the implementation of the Weight dropout technique.\n",
        "    The class is taken from: https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/weight_drop.html\n",
        "    '''\n",
        "    def __init__(self, module, weights, dropout=0, variational=False):\n",
        "        super(WeightDrop, self).__init__()\n",
        "        self.module = module\n",
        "        self.weights = weights\n",
        "        self.dropout = dropout\n",
        "        self.variational = variational\n",
        "        self._setup()\n",
        "        self.hooker = BackHook(lambda: self._backward())\n",
        "\n",
        "    def _setup(self):\n",
        "        for name_w in self.weights:\n",
        "            w = getattr(self.module, name_w)\n",
        "            self.register_parameter(name_w + '_raw', Parameter(w.data))\n",
        "\n",
        "    def _setweights(self):\n",
        "        for name_w in self.weights:\n",
        "            raw_w = getattr(self, name_w + '_raw')\n",
        "            if self.training:\n",
        "                mask = raw_w.new_ones((raw_w.size(0), 1))\n",
        "                mask = torch.nn.functional.dropout(mask, p=self.dropout, training=True)\n",
        "                w = mask.expand_as(raw_w) * raw_w\n",
        "                setattr(self, name_w + \"_mask\", mask)\n",
        "            else:\n",
        "                w = raw_w\n",
        "            rnn_w = getattr(self.module, name_w)\n",
        "            rnn_w.data.copy_(w)\n",
        "\n",
        "    def _backward(self):\n",
        "        # transfer gradients from embeddedRNN to raw params\n",
        "        for name_w in self.weights:\n",
        "            raw_w = getattr(self, name_w + '_raw')\n",
        "            rnn_w = getattr(self.module, name_w)\n",
        "            raw_w.grad = rnn_w.grad * getattr(self, name_w + \"_mask\")\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._setweights()\n",
        "        return self.module(*self.hooker(*args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSbQxaQNTq51"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class EmbeddingDropout(torch.nn.Embedding):\n",
        "    '''\n",
        "    Class for the implementation of Embedding dropout at word level.\n",
        "    This class is taken from: https://github.com/salesforce/awd-lstm-lm/blob/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5/embed_regularize.py#L5\n",
        "    '''\n",
        "    def __init__(self, num_embeddings, embedding_dim, padding_idx=0, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False, dropout=0.1, scale=None):\n",
        "        nn.Embedding.__init__(self, num_embeddings = num_embeddings, embedding_dim = embedding_dim, padding_idx = padding_idx, max_norm = max_norm, norm_type = norm_type, scale_grad_by_freq = scale_grad_by_freq, sparse = sparse)\n",
        "        self.dropout = dropout\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.training:\n",
        "            dropout = self.dropout\n",
        "        else:\n",
        "            dropout = 0\n",
        "\n",
        "        if dropout:\n",
        "            mask = self.weight.data.new(self.weight.size(0), 1).bernoulli_(1 - dropout).expand_as(self.weight)/ (1 - dropout)\n",
        "            masked_weight = self.weight * Variable(mask)\n",
        "        else:\n",
        "            masked_weight = self.weight\n",
        "        if self.scale and self.scale != 1:\n",
        "            masked_weight = masked_weight * self.scale\n",
        "\n",
        "        return F.embedding(inputs, masked_weight, max_norm=self.max_norm, norm_type=self.norm_type, scale_grad_by_freq=self.scale_grad_by_freq, sparse=self.sparse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETwK-rznN6zB"
      },
      "source": [
        "# LSTM with improvements\n",
        "\n",
        "As suggested by the paper \"Merity, S., Keskar, N. S., & Socher, R. (2017). Regularizing and Optimizing LSTM Language Models. arXiv. https://doi.org/10.48550/arXiv.1708.02182\".\n",
        "\n",
        "The improvements that I applied are:\n",
        "- Weight dropout for the recurrent hidden to hidden weight matrices, in order to prevent the overfitting of the LSTM. It is important to notice that the dropped weights remain dropped for the entirety of the forward and backward pass.\n",
        "\n",
        "- NT-ASGD: this optimizer is the Non-monotonically Triggered ASGD. It is an improved version of the ASGD optimizer.\n",
        "\n",
        "- Variational dropout, which is a binary dropout mask only once upon the first call and then to repeatedly use that locked dropout mask for all repeated connections within the forward and backward pass.\n",
        "\n",
        "- Embedding dropout is the application of dropout on the embedding matrix at world level\n",
        "\n",
        "- Weight tying, which shares the weights between the embedding and softmax layer, substantially reducing the total parameter count in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_UQHO9ynHHZ"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch import nn\n",
        "from typing import List, Tuple, Union\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class FinalModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This is the implementation of the LSTM with the improvements proposed by the paper \n",
        "    \"Merity, S., Keskar, N. S., & Socher, R. (2017). Regularizing and Optimizing LSTM Language Models.\n",
        "    arXiv. https://doi.org/10.48550/arXiv.1708.02182\".\n",
        "        num_classes: number of classes\n",
        "        emb_dim: embedding dimension for the feature representation of the words\n",
        "        hid_dim: dimension of the hidden layer of the LSTM\n",
        "        weight_d: weight dropout of the hidden to hidden weight matrices of the LSTM\n",
        "        input_d: variational dropout applied to the input\n",
        "        output_d: variational dropout applied to the output\n",
        "        tie_weights: bool for the application of the weight tying\n",
        "        emb_dropout: value for the embedding dropout\n",
        "        n_layers: number of hidden layers of the LSTM\n",
        "        pad_value: value used for the padding of the sequences\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, emb_dim, hid_dim, weight_d = None, input_d = None, output_d = None, tie_weights = False, emb_dropout = None, n_layers = 3, pad_value = 0):\n",
        "        super(FinalModel, self).__init__()\n",
        "\n",
        "        if (emb_dropout is not None):\n",
        "          self.embd = EmbeddingDropout(num_classes, emb_dim, pad_value, dropout = emb_dropout)\n",
        "        else:\n",
        "          self.embd = nn.Embedding(num_classes, emb_dim, pad_value)\n",
        "\n",
        "        self.embd.weight.data.uniform_(-0.1, 0.1)\n",
        "        \n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, bidirectional=False)\n",
        "        self.classifier = nn.Linear(hid_dim, num_classes)\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hid_dim\n",
        "        self.pad_value = pad_value\n",
        "        self.init_weights()\n",
        "\n",
        "        self.weight_d = weight_d\n",
        "        self.input_d = input_d\n",
        "        self.output_d = output_d\n",
        "        self.tie_weights = tie_weights\n",
        "        self.emb_dropout = emb_dropout\n",
        "\n",
        "        hh_weights = []\n",
        "\n",
        "        if weight_d is not None:\n",
        "          for i in range(n_layers):\n",
        "              hh_weights.append('weight_hh_l{}'.format(i))\n",
        "          self.lstm = WeightDrop(self.lstm, hh_weights, weight_d)  #apply weight dropout to the hidden to hidden weights\n",
        "          \n",
        "        if (input_d is not None) and (output_d is not None):\n",
        "          self.input_drop = LockedDropout(input_d)\n",
        "          self.output_drop = LockedDropout(output_d)\n",
        "        elif (input_d != output_d):\n",
        "          raise ValueError(\"Input and Output dropout must be the same\")\n",
        "\n",
        "        if tie_weights:\n",
        "            if hid_dim != emb_dim:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.classifier.weight = self.embd.weight\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.data.ndimension() >= 2:\n",
        "                nn.init.xavier_uniform_(p.data)\n",
        "            else:\n",
        "                nn.init.zeros_(p.data)\n",
        "\n",
        "    def forward(self, input, lengths, hidden = None):        \n",
        "        total_length = input.shape[1]\n",
        "        \n",
        "        embedding = self.embd(input) #apply embedding dropout\n",
        "        \n",
        "        if (self.input_d is not None) and (self.output_d is not None): \n",
        "          embedding = self.input_drop(embedding)\n",
        "\n",
        "        packed_input = pack_padded_sequence(embedding, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hidden = self.lstm(packed_input)\n",
        "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True, padding_value=self.pad_value, total_length=total_length)\n",
        "\n",
        "        if (self.input_d is not None) and (self.output_d is not None): \n",
        "          output = self.output_drop(output)\n",
        "\n",
        "        output = self.classifier(output)\n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        output = output.reshape(output.size(0)*output.size(1), output.size(2))\n",
        "\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5N0d68_Nm0i"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def main_finalmodel(hid_size, emb_size, weight_d, input_d, output_d, tie_weights, emb_dropout, ntsgd, n_layers, lr, weight_decay, clip, pad_value, epochs):\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    data_train = read_file('dataset/ptb.train.txt')\n",
        "    vocab = Lang(data_train)\n",
        "\n",
        "    train_dataset = Dataset('dataset/ptb.train.txt', vocab)\n",
        "    test_dataset = Dataset('dataset/ptb.test.txt', vocab)\n",
        "    valid_dataset = Dataset('dataset/ptb.valid.txt', vocab)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "    vocab_len = len(vocab.word2id)\n",
        "\n",
        "    model = FinalModel(vocab_len, emb_size, hid_size, weight_d, input_d, output_d, tie_weights, emb_dropout, n_layers, pad_value).to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=pad_value)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "    wandb.init(project=\"NLU_project\",\n",
        "               name = \"LSTM no Variational dropout\",\n",
        "               config = {\n",
        "               \"learning_rate\": lr,\n",
        "               \"architecture\": \"FinalModel\",\n",
        "               \"hidden_size\": hid_size,\n",
        "               \"embedding_size\": emb_size,\n",
        "               \"weight_dropout\": weight_d,\n",
        "               \"input_dropout\": input_d,\n",
        "               \"input_dropout\": output_d,\n",
        "               \"tie_weights\": tie_weights,\n",
        "               \"embedding_dropout\": emb_dropout,\n",
        "               \"num_layers\": n_layers,\n",
        "               \"epochs\": epochs},\n",
        "               notes = \"LSTM no Variational dropout\"\n",
        "               )\n",
        "    \n",
        "    train(model, optimizer, loss_function, train_loader, test_loader, valid_loader, epochs, device, clip, scheduler, ntsgd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RifsNm0rQC5i"
      },
      "outputs": [],
      "source": [
        "main_finalmodel(hid_size = 500, emb_size = 500, weight_d = 0.5, input_d = None, output_d = None, tie_weights = True, emb_dropout = 0.1, ntsgd=True, n_layers = 3, lr = 10.0, weight_decay = 1.2e-6, clip=1, pad_value = 0, epochs = 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uACC0IfTTq53"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb4Y6ghJTq54"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "def transformer_init(model_id, device):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
        "    \n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlvw_Es7Tq54"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def get_data(data_name, tokenizer):\n",
        "    ptb_train = load_dataset(data_name, split=\"train\")\n",
        "    ptb_test = load_dataset(data_name, split=\"test\")\n",
        "    ptb_valid = load_dataset(data_name, split=\"validation\")\n",
        "\n",
        "    ptb_train_encodings = tokenizer(\"\".join(ptb_train[\"sentence\"]), return_tensors=\"pt\")\n",
        "    ptb_test_encodings = tokenizer(\"\".join(ptb_test[\"sentence\"]), return_tensors=\"pt\")\n",
        "    ptb_valid_encodings = tokenizer(\"\".join(ptb_valid[\"sentence\"]), return_tensors=\"pt\")\n",
        "\n",
        "    return ptb_train_encodings, ptb_test_encodings, ptb_valid_encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywCYqRLCTq55"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_step_transformer(model, optimizer, ptb_train_encodings, device, stride):\n",
        "    model.train()\n",
        "    max_length = model.config.n_positions\n",
        "    seq_len = ptb_train_encodings.input_ids.size(1)\n",
        "    \n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
        "        end_loc = min(begin_loc + max_length, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc\n",
        "        input_ids = ptb_train_encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        input_ids.to(device)\n",
        "        target_ids.to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        neg_log_likelihood = outputs.loss * trg_len\n",
        "        \n",
        "        neg_log_likelihood.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        nlls.append(neg_log_likelihood)\n",
        "    \n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len:\n",
        "            break\n",
        "        ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iikT-u-OTq55"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def test_step_transformer(model, ptb_test_encodings, device, stride):\n",
        "    max_length = model.config.n_positions\n",
        "    seq_len = ptb_test_encodings.input_ids.size(1)\n",
        "\n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
        "        end_loc = min(begin_loc + max_length, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc\n",
        "        input_ids = ptb_test_encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "            neg_log_likelihood = outputs.loss * trg_len\n",
        "\n",
        "        nlls.append(neg_log_likelihood)\n",
        "\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len:\n",
        "            break\n",
        "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
        "    \n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lMKLTJmTq56"
      },
      "outputs": [],
      "source": [
        "from pytorch_transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "\n",
        "def finetuning(model, tokenizer, optimizer, ptb_train_encodings, ptb_test_encodings, ptb_valid_encodings, epochs, stride, device):\n",
        "    perplexity = float('inf')\n",
        "    valid_perplexity = float('inf')\n",
        "    \n",
        "    for e in tqdm(range(epochs)):\n",
        "        train_perplexity = train_step_transformer(model, optimizer, ptb_train_encodings, device, stride)\n",
        "        test_perplexity = test_step_transformer(model, ptb_test_encodings, device, stride)\n",
        "        valid_perplexity = test_step_transformer(model, ptb_valid_encodings, device, stride)\n",
        "\n",
        "        print(\"\\nEpoch: {}\".format(e+1))\n",
        "        print(\"Train Perplexity : {}\" .format(train_perplexity))\n",
        "        print(\"Test Perplexity : {}\" .format(test_perplexity))\n",
        "        print(\"Valid Perplexity : {}\" .format(valid_perplexity))\n",
        "\n",
        "        if valid_perplexity < perplexity:\n",
        "            os.makedirs(\"/content/gpt2_weights/\", exist_ok = True)\n",
        "            output_dir = \"/content/gpt2_weights/\"\n",
        "            model_to_save = model.module if hasattr(model, 'module') else model\n",
        "\n",
        "            output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "            output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "\n",
        "            torch.save(model_to_save.state_dict(), output_model_file)\n",
        "            model_to_save.config.to_json_file(output_config_file)\n",
        "            tokenizer.save_vocabulary(output_dir)\n",
        "\n",
        "            perplexity = valid_perplexity\n",
        "        \n",
        "        wandb.log({\"Epoch\": e+1, \"train_perplexity\": train_perplexity, \"test_perplexity\": test_perplexity, \"valid_perplexity\": valid_perplexity})\n",
        "    wandb.log({'best_perplexity': perplexity})\n",
        "    wandb.run.summary[\"best_perplexity\"] = perplexity\n",
        "    wandb.finish()\n",
        "            \n",
        "    print(\"\\Finetuning completed, the best perplexity reached in validation is : {}\".format(perplexity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQaqnvShTq56"
      },
      "outputs": [],
      "source": [
        "def main_transformer_finetuning(model_name, device, data_name, stride, lr, wd, epochs):\n",
        "    model, tokenizer = transformer_init(model_name, device)\n",
        "    train_data, test_data, eval_data = get_data(data_name, tokenizer)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    wandb.init(project=\"NLU_project\",\n",
        "               name = \"Tranformer GPT2 Finetuning\",\n",
        "               config = {\n",
        "               \"learning_rate\": lr,\n",
        "               \"architecture\": \"GPT2\",\n",
        "               \"stride\": stride,\n",
        "               \"learning_rate\": lr,\n",
        "               \"weight_decay\": wd,\n",
        "               \"epochs\": epochs},\n",
        "               notes = \"run for the finetuning of GPT2\"\n",
        "               )\n",
        "\n",
        "    finetuning(model, tokenizer, optimizer, train_data, test_data, eval_data, epochs, stride, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsT2ncWuTq57"
      },
      "outputs": [],
      "source": [
        "main_transformer_finetuning(model_name=\"gpt2\", device=\"cuda\", data_name=\"ptb_text_only\", stride=1024, lr=2e-05, wd=0.01, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2pcHik9Tq57"
      },
      "outputs": [],
      "source": [
        "def main_tranformer_zero_shot(model_name, device, data_name, stride):\n",
        "    model, tokenizer = transformer_init(model_name, device)\n",
        "    train_data, test_data, eval_data = get_data(data_name, tokenizer)\n",
        "\n",
        "    test_perplexity = test_step_transformer(model, test_data, device, stride)\n",
        "    valid_perplexity = test_step_transformer(model, eval_data, device, stride)\n",
        "\n",
        "    wandb.init(project=\"NLU_project\",\n",
        "               name = \"Tranformer GPT2 Zero-shot\",\n",
        "               config = {\n",
        "               \"architecture\": \"GPT2\",\n",
        "               \"stride\": stride\n",
        "               },\n",
        "               notes = \"run for zero-shot GPT2\"\n",
        "               )\n",
        "\n",
        "    wandb.log({'best_perplexity': valid_perplexity})\n",
        "    wandb.run.summary[\"best_perplexity\"] = valid_perplexity\n",
        "    wandb.finish()\n",
        "\n",
        "    print(\"\\nZero-shot GPT2 test perplexity on PTB is:{}\".format(test_perplexity))\n",
        "    print(\"\\nZero-shot GPT2 validation perplexity on PTB is:{}\".format(valid_perplexity))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OIk1rxTTq58"
      },
      "outputs": [],
      "source": [
        "main_tranformer_zero_shot(model_name=\"gpt2\", device=\"cuda\", data_name=\"ptb_text_only\", stride=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SVXG6ezTq58"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prediction_transformer(model_weights, tokenizer_weights, sentence_prompt, max_tokens):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_weights)\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_weights)\n",
        "\n",
        "    inputs = tokenizer(sentence_prompt, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(inputs, max_new_tokens=max_tokens, do_sample=True, top_k=50, top_p=0.95)\n",
        "    \n",
        "    final_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    print(final_sentence)\n",
        "\n",
        "prediction_transformer(model_weights=\"gpt2\", tokenizer_weights=\"gpt2\", sentence_prompt=\"the new plan is\", max_tokens=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8D3nD8QnGFQ"
      },
      "source": [
        "# Model Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgSk-axBTq59"
      },
      "source": [
        "Method for the count of the trainable parameters of the LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4KdXTclTq59"
      },
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "  table = PrettyTable([\"Mod name\", \"Parameters Listed\"])\n",
        "  t_params = 0\n",
        "  for name, parameter in model.named_parameters():\n",
        "    if not parameter.requires_grad: continue\n",
        "    param = parameter.numel()\n",
        "    table.add_row([name, param])\n",
        "    t_params+=param\n",
        "  print(table)\n",
        "  print(f\"Sum of trained paramters: {t_params}\")\n",
        "\n",
        "data_train = read_file('dataset/ptb.train.txt')\n",
        "vocab = Lang(data_train)\n",
        "count_parameters(BaselineLSTM(num_classes=len(vocab.word2id), emb_dim=300, hid_dim=300, n_layers=1, pad_value=0))\n",
        "count_parameters(FinalModel(num_classes=len(vocab.word2id), emb_dim=500, hid_dim=500, weight_d=0.5, input_d=0.4, output_d=0.4, tie_weights=True, emb_dropout=0.1, n_layers=3, pad_value=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqSAB8XDTq5-"
      },
      "source": [
        "Method used to evaluate the performance of the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIZgt-jmTq5-"
      },
      "outputs": [],
      "source": [
        "def prediction_lstm(model, weights_path, sentence, vocab, num_words_to_predict, device):\n",
        "  \n",
        "  model.load_state_dict(torch.load(weights_path))\n",
        "  \n",
        "  input = []\n",
        "  length = []\n",
        "\n",
        "  sentence_words = read_sentence(sentence)\n",
        "  words = [vocab.word2id[word] for word in sentence_words]\n",
        "\n",
        "  input.append(words)\n",
        "  length.append(len(words))\n",
        "\n",
        "  input = torch.LongTensor(input).to(device)\n",
        "  model = model.to(device)\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for i in range(num_words_to_predict):\n",
        "          output, hidden = model(input, length)\n",
        "\n",
        "          predicted_id_word = output.max(dim=1).indices[-1].tolist()\n",
        "          predicted_word = vocab.id2word[predicted_id_word]\n",
        "          if predicted_word == \"<eos>\":\n",
        "            break\n",
        "\n",
        "          input = torch.cat((input, torch.tensor([[vocab.word2id[predicted_word]]], dtype=torch.long)),1)\n",
        "          length = [input.shape[1]]\n",
        "\n",
        "        \n",
        "  final_sentence = [vocab.id2word[words.tolist()] for words in input.flatten()]\n",
        "  print(final_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcAwFanATq5_"
      },
      "outputs": [],
      "source": [
        "data_train = read_file('dataset/ptb.train.txt')\n",
        "vocab = Lang(data_train)\n",
        "device = \"cpu\"\n",
        "model = BaselineLSTM(len(vocab.word2id), 300, 300, 1, 0)\n",
        "prediction_lstm(model, \"/content/lstm_weights/baseline_weights.pt\", \"once upon a time\", vocab, 15, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmFLLpzRBah2"
      },
      "source": [
        "# Dataset Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3qyeQMrJtuD"
      },
      "outputs": [],
      "source": [
        "class Dataset_analysis():\n",
        "  def __init__(self, dataset_path, data):\n",
        "    self.corpus = self.read_document(dataset_path)\n",
        "    self.num_words = self.num_word_corpus(self.corpus)\n",
        "    self.num_sentence = self.num_sentence_corpus(self.corpus)\n",
        "    self.num_words_sent = self.num_word_sentence(self.corpus)\n",
        "    self.num_chars_sent = self.num_char_sentence(self.corpus)\n",
        "    self.num_chars_word = self.num_char_words(self.corpus)\n",
        "    self.analysis(data)\n",
        "\n",
        "  def read_document(self, file):\n",
        "    with open(file, 'r') as f:\n",
        "        data = f.read()\n",
        "    return data\n",
        "\n",
        "  def num_word_corpus(self, doc):\n",
        "    return len(doc.split())\n",
        "  \n",
        "  def num_sentence_corpus(self, doc):\n",
        "    return len(doc.splitlines())\n",
        "  \n",
        "  def num_word_sentence(self, doc):\n",
        "    return len(doc.split())/len(doc.splitlines())\n",
        "  \n",
        "  def num_char_sentence(self, doc):\n",
        "    return len(doc)/len(doc.splitlines())\n",
        "\n",
        "  def num_char_words(self, doc):\n",
        "    return len(doc)/len(doc.split())\n",
        "\n",
        "  def analysis(self, data):\n",
        "    print(f\"Number of words in {data} dataset: {self.num_words}\")\n",
        "    print(f\"Number of sentences in {data} dataset: {self.num_sentence}\")\n",
        "    print(f\"Average words per sentence in {data} dataset: {self.num_words_sent}\")\n",
        "    print(f\"Average chars per word in {data} dataset: {self.num_chars_word}\")\n",
        "    print(f\"Average chars per sentence in {data} dataset: {self.num_chars_sent}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYwowdvQMp5q"
      },
      "outputs": [],
      "source": [
        "dataset_train = Dataset_analysis('/content/dataset/ptb.train.txt', 'train')\n",
        "dataset_test = Dataset_analysis('/content/dataset/ptb.test.txt', 'test')\n",
        "dataset_valid = Dataset_analysis('/content/dataset/ptb.valid.txt', 'validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVPHPVEbTq6A"
      },
      "outputs": [],
      "source": [
        "def unk_n_hashtag_occ(dataset, text):\n",
        "    text = text.split()\n",
        "    counter = Counter(text)\n",
        "\n",
        "    print(\"For {} set occurrences of the token unk are {}\".format(dataset, counter[\"<unk>\"]))\n",
        "    print(\"For {} set occurrences of the token N are {}\".format(dataset, counter[\"N\"]))\n",
        "    print(\"For {} set occurrences of the token $ are {}\\n\".format(dataset, counter[\"$\"]))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCsWn0qqTq6A"
      },
      "outputs": [],
      "source": [
        "unk_n_hashtag_occ(\"train\", dataset_train.corpus)\n",
        "unk_n_hashtag_occ(\"test\", dataset_test.corpus)\n",
        "unk_n_hashtag_occ(\"validation\", dataset_valid.corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZAdAe6kBthD"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from collections import  Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def plot_top_non_stopwords_barchart(text):\n",
        "    stop=set(stopwords.words('english'))\n",
        "    stop.add('<unk>')\n",
        "    stop.add('N')\n",
        "    stop.add('$')\n",
        "    text = text.split()\n",
        "\n",
        "    counter=Counter(text)\n",
        "    most=counter.most_common()\n",
        "    x, y=[], []\n",
        "    for word,count in most[:40]:\n",
        "        if (word not in stop):\n",
        "            x.append(word)\n",
        "            y.append(count)\n",
        "    fig, ax = plt.subplots(figsize = (10, 8))\n",
        "    sns.barplot(x=y,y=x, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPphPkuvB1db"
      },
      "outputs": [],
      "source": [
        "plot_top_non_stopwords_barchart(dataset_train.corpus)\n",
        "plot_top_non_stopwords_barchart(dataset_test.corpus)\n",
        "plot_top_non_stopwords_barchart(dataset_valid.corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU8sym96B9fH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_top_stopwords_barchart(text):\n",
        "    stop=set(stopwords.words('english'))\n",
        "\n",
        "    text = text.split() #conver to list\n",
        "\n",
        "    counter=Counter(text)\n",
        "    most=counter.most_common()\n",
        "    x, y=[], []\n",
        "    for word,count in most[:40]:\n",
        "        if (word in stop):\n",
        "            x.append(word)\n",
        "            y.append(count)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize = (10, 8))\n",
        "    x = x[0:8]\n",
        "    y = y[0:8]\n",
        "    sns.barplot(x=y,y=x, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5__URdQCAyX"
      },
      "outputs": [],
      "source": [
        "plot_top_stopwords_barchart(dataset_train.corpus)\n",
        "plot_top_stopwords_barchart(dataset_test.corpus)\n",
        "plot_top_stopwords_barchart(dataset_valid.corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7eCy3gAB22G"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def average_sentence_length_plot(doc):\n",
        "    sentence_length = []\n",
        "    for sentence in doc.splitlines():\n",
        "        sentence_length.append(len(sentence.split()))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
        "    sns.histplot(sentence_length, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J5S_TgzB4R_"
      },
      "outputs": [],
      "source": [
        "average_sentence_length_plot(dataset_train.corpus)\n",
        "average_sentence_length_plot(dataset_test.corpus)\n",
        "average_sentence_length_plot(dataset_valid.corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ1iePSjB5PC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def average_word_length_plot(doc):\n",
        "    word_length = []\n",
        "    for word in doc.split():\n",
        "        word_length.append(len(word))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
        "    sns.histplot(word_length, ax = ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8fcB3VAB6es"
      },
      "outputs": [],
      "source": [
        "average_word_length_plot(dataset_train.corpus)\n",
        "average_word_length_plot(dataset_test.corpus)\n",
        "average_word_length_plot(dataset_valid.corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLwNufjGB7ed"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "def get_top_ngrams(corpus, stop_words, n=None):\n",
        "\n",
        "    sw = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
        "\n",
        "    vec = CountVectorizer(ngram_range=(n, n), stop_words=sw).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:10]\n",
        "\n",
        "def plot_top_ngrams(corpus, stop_words, n=None):\n",
        "    top_n_bigrams=get_top_ngrams(corpus, stop_words, n)\n",
        "    x,y=map(list,zip(*top_n_bigrams))\n",
        "    fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
        "    sns.barplot(x=y,y=x, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZzhJt3FCDhs"
      },
      "outputs": [],
      "source": [
        "plot_top_ngrams([dataset_train.corpus], ['unk'], n=3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "25dde02789f54a18092ae561e33a6442fcd50f97db1e4e99b6c603ea15954209"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
